"""
Adaptive coaching system for the Learn-As-You-Go Code Review Assistant.
FIXED: MCQ display issues for Streamlit Cloud deployment.
"""

import uuid
from typing import Optional, Dict, Any, Tuple
from .coaching_models import (
    CoachingState, CoachingInteraction, CoachingMode, 
    AnswerStatus, LearningQuestion, QuestionType
)
from .question_templates import QuestionSelector, QuestionTemplates
from .analyzer import CodeAnalyzer
from templates.examples import ExampleGenerator, get_example_code


class AdaptiveCoach:
    """
    Main coaching system that decides between asking questions or giving nudges.
    Tracks user progress and adapts teaching strategy accordingly.
    """
    
    def __init__(self, code_analyzer: CodeAnalyzer):
        print("DEBUG: AdaptiveCoach instance created.")

        self.code_analyzer = code_analyzer
        self.question_selector = QuestionSelector()
        self.first_example_shown = False  # Track whether the first example has been shown
        self.first_example_code = None  # Store the first example code
        print("DEBUG: Initial state - first_example_shown:", self.first_example_shown)
        print("DEBUG: Initial state - first_example_code:", self.first_example_code)
    
    def load_example_code(self) -> Tuple[str, str]:
        """
        Returns a tuple of (code_snippet, category).

        On the very first call, returns the fixed example via get_example_code().
        On each subsequent call, delegates to ExampleGenerator.get_random_example().
        """
        # First call: show the hard‚Äêcoded example, then flip the flag
        if not self.first_example_shown:
            self.first_example_shown = True
            self.first_example_code = get_example_code()
            return self.first_example_code, "performance"

        # Second+ calls: fetch a random example (excluding the first)
        example_code, category = ExampleGenerator.get_random_example(
            exclude_code=self.first_example_code
        )
        # Avoid returning the same snippet twice
        while example_code == self.first_example_code:
            example_code, category = ExampleGenerator.get_random_example(
                exclude_code=self.first_example_code
            )
        return example_code, category
    
    def detect_main_issue_with_claude(self, code: str) -> str:
        """
        Use Claude/LLM to detect the main issue or learning goal in the user's code.
        Returns a short keyword (e.g., 'vectorization', 'readability', 'no_issue').
        """
        prompt = (
            "Analyze the following Python code and identify the single most important optimization or learning opportunity for the user. "
            "Respond with a short keyword (e.g., 'vectorization', 'readability', 'no_issue').\n\n"
            f"Code:\n{code}"
        )
        # Make sure you have access to an API client for Claude
        return self.code_analyzer.api_client.call_claude(prompt).strip().lower()
    
    def process_code_submission(self, code: str, coaching_state: CoachingState) -> Tuple[str, CoachingMode]:
        print("Processing code submission...")
        code_analysis = self._analyze_code_for_coaching(code)

        # Set main issue if not already set
        if not coaching_state.main_issue:
            if code_analysis['has_iterrows']:
                coaching_state.main_issue = 'has_iterrows'
            elif code_analysis['has_string_concat']:
                coaching_state.main_issue = 'has_string_concat'
            elif code_analysis['has_nested_loops']:
                coaching_state.main_issue = 'has_nested_loops'
            else:
                coaching_state.main_issue = self.detect_main_issue_with_claude(code)
        print("DEBUG: main_issue =", coaching_state.main_issue)
        # Map issue keys to analysis flags
        issue_flags = {
            'has_iterrows': code_analysis.get('has_iterrows', False),
            'has_string_concat': code_analysis.get('has_string_concat', False),
            'has_nested_loops': code_analysis.get('has_nested_loops', False),
            # Add more as needed
        }
        print("DEBUG: code_analysis =", code_analysis)
        print("DEBUG: issue_flags =", issue_flags)
        # Only show congratulations if the main issue was previously present and is now resolved
        main_issue = coaching_state.main_issue
        if main_issue and main_issue in issue_flags and not issue_flags[main_issue]:
            print("DEBUG: Exiting early due to resolved main issue")
            print("DEBUG: main_issue =", main_issue)
            print("DEBUG: issue_flags[main_issue] =", issue_flags.get(main_issue))
            coaching_state.resolved_issues.add(main_issue)
            return """üéâ **Congratulations!** You've solved the main problem for this example.

            Would you like to:
            ‚Ä¢ Paste in your own code to analyze?
            ‚Ä¢ Or type 'example' to load a new sample code snippet?

            Let me know how you'd like to proceed!""", CoachingMode.NUDGE
            
        # Otherwise, proceed as before
        print("Questions asked:", coaching_state.total_questions_asked)
        print("Success rate:", coaching_state.get_success_rate())
        print("Should ask question?", self.question_selector.should_ask_question(coaching_state, code_analysis))
        should_question = self.question_selector.should_ask_question(coaching_state, code_analysis)
        if should_question:
            return self._create_learning_question(code, coaching_state, code_analysis)
        else:
            return self._create_nudge(code, code_analysis, coaching_state)

    
    def handle_user_answer(self, user_answer: str, coaching_state: CoachingState) -> str:
        """
        Process user's answer with intelligent decision-making between hints and questions.
        
        Args:
            user_answer: The user's response
            coaching_state: Current coaching state
            
        Returns:
            Feedback message for the user
        """
        user_answer_lower = user_answer.strip().lower()
        
        # Handle explicit requests first
        if user_answer_lower in ['hint', 'give me a hint', 'need a hint']:
            return self._provide_contextual_hint(coaching_state)
        
        if user_answer_lower in ['explore', 'explore further', 'tell me more']:
            return self._provide_exploration_guidance(coaching_state)
        
        if user_answer_lower == 'example':
            print("DEBUG: 'example' command received.")
            example_code, category = self.load_example_code()
            print("DEBUG: Example loaded:", example_code, category)
            return f"Example loaded! Here's a {category} example:\n\n```python\n{example_code}\n```"
        # If waiting for answer to a specific question
        if coaching_state.is_waiting_for_answer():
            current_question = coaching_state.current_interaction.question
            is_correct, base_feedback = self._evaluate_answer(user_answer, current_question)
            
            # Complete the interaction
            status = AnswerStatus.CORRECT if is_correct else AnswerStatus.INCORRECT
            coaching_state.complete_current_interaction(user_answer, status)
            
            # Create clean, intelligent response (no concatenation)
            if is_correct:
                return self._create_clean_correct_response(base_feedback, coaching_state)
            else:
                return self._create_clean_incorrect_response(base_feedback, coaching_state)
        
        # General conversation - not waiting for specific answer
        return self._handle_general_conversation(user_answer, coaching_state)
    
    def _create_clean_correct_response(self, base_feedback: str, coaching_state: CoachingState) -> str:
        """Create clean, intelligent response for correct answers without duplication."""
        # Extract core feedback without generic next steps
        lines = base_feedback.split('\n')
        clean_parts = []
        
        for line in lines:
            # Keep the main feedback but skip redundant next steps
            if any(skip_phrase in line for skip_phrase in [
                'Next step:', 'Submit your improved', 'ask for a hint', 
                'when ready', 'if you need guidance'
            ]):
                break
            clean_parts.append(line)
        
        # Build clean response with intelligent next step
        result = '\n'.join(clean_parts).strip()
        
        # Add single, intelligent next step based on user performance
        success_rate = coaching_state.get_success_rate()
        
        if success_rate > 0.8 and coaching_state.total_questions_asked >= 2:
            # High performer - advanced challenge
            result += "\n\nüéØ **Advanced Challenge:** Now that you understand this concept, what do you think happens to performance when you have nested loops processing large datasets? Can you predict the time complexity?"
            result += "\n\nüí° **Learning Options:** Want a hint to guide your thinking, or prefer another hands-on question to test your understanding?"
        elif success_rate > 0.6:
            # Good performer - practical application
            result += "\n\nüîß **Apply It:** Try modifying your code to use vectorized operations. Replace the loop with direct column operations and submit your improved version!"
            result += "\n\nüí° **Learning Options:** Need a hint on vectorized syntax, or want to explore this concept with another question first?"
        else:
            # Building confidence - supportive guidance
            result += "\n\n‚ú® **Great progress!** Now try changing just the calculation part to work with entire columns instead of individual rows. You've got this!"
            result += "\n\nüí° **Learning Options:** Ask for a hint if you need guidance, or request another question to practice this concept more!"
        
        return result
    
    def _create_clean_incorrect_response(self, base_feedback: str, coaching_state: CoachingState) -> str:
        """Create clean, supportive response for incorrect answers."""
        # Extract core feedback without generic encouragement
        lines = base_feedback.split('\n')
        clean_parts = []
        
        for line in lines:
            # Keep explanation but skip generic encouragement
            if any(skip_phrase in line for skip_phrase in [
                'Keep learning:', 'try another question', 'ask for a hint about'
            ]):
                break
            clean_parts.append(line)
        
        result = '\n'.join(clean_parts).strip()
        
        # Add intelligent guidance based on struggle pattern
        recent_interactions = coaching_state.interaction_history[-3:] if coaching_state.interaction_history else []
        incorrect_count = sum(1 for i in recent_interactions if i.answer_status == AnswerStatus.INCORRECT)
        
        if incorrect_count >= 2:
            # Multiple wrong answers - concrete help
            result += "\n\nüí° **Concrete Hint:** Replace `for idx, row in df.iterrows():` with direct operations like `df['new_column'] = df['price'] * 0.2 + df['tax']`. This works on entire columns at once!"
            result += "\n\nüéì **Learning Options:** Want to try applying this directly, or explore more with another question to solidify your understanding?"
        else:
            # Single wrong answer - conceptual encouragement
            result += "\n\nü§î **Think about this:** Pandas is designed to work with entire columns of data. Instead of processing one row at a time, what if you could do the math on all rows simultaneously?"
            result += "\n\nüí° **Learning Options:** Ask for a more specific hint, or try another question to explore this concept further!"
        
        return result
    
    def _provide_contextual_hint(self, coaching_state: CoachingState) -> str:
        """Provide hint based on current context and user progress."""
        # Check what they're struggling with
        recent_interactions = coaching_state.interaction_history[-3:] if coaching_state.interaction_history else []
        incorrect_count = sum(1 for i in recent_interactions 
                             if i.answer_status == AnswerStatus.INCORRECT)
        
        if incorrect_count >= 2:
            # User is struggling - provide concrete hint
            return "üí° **Specific Help:** Look at your loop - instead of `for idx, row in df.iterrows():`, try `df['result'] = df['column1'] * value + df['column2']`. This processes all rows at once!"
        elif coaching_state.get_success_rate() > 0.7:
            # User doing well - provide conceptual hint
            return "üß† **Conceptual Hint:** Think about the difference between imperative (telling the computer how to do each step) and declarative (telling it what you want). Pandas excels at declarative operations."
        else:
            # Balanced approach
            return "üí≠ **Balanced Hint:** Consider that pandas can perform mathematical operations on entire columns. What would happen if you applied your calculation directly to the columns?"
    
    def _provide_exploration_guidance(self, coaching_state: CoachingState) -> str:
        """Provide exploration guidance based on current learning context."""
        # If they just answered correctly, dive deeper
        if (coaching_state.interaction_history and 
            coaching_state.interaction_history[-1].answer_status == AnswerStatus.CORRECT):
            return "üîç **Deeper Question:** Great question! Now think about this: Why do you think vectorized operations are faster? What's happening under the hood that makes column operations more efficient than loops?"
        
        # If they're exploring without specific context, guide them
        return "üéØ **Let's Explore:** That's a great mindset! Let's dive into the core concept: How do you think pandas handles operations on entire columns vs. processing one row at a time?"
    
    def _handle_general_conversation(self, user_answer: str, coaching_state: CoachingState) -> str:
        """Handle general conversation when not waiting for specific answer."""
        return "I'm here to help with your code! Please submit some code to get started, or ask a specific question about optimization."
    
    # PRESERVED ORIGINAL METHODS - All existing functionality maintained
    
    def _analyze_code_for_coaching(self, code: str) -> Dict[str, Any]:
        """Analyze code to identify coaching opportunities."""
        analysis = {
            'has_iterrows': 'iterrows' in code.lower(),
            'has_string_concat': '+=' in code and any(s in code.lower() for s in ['str', '"', "'"]),
            'has_nested_loops': code.count('for ') > 1,
            'line_count': len(code.split('\n')),
            'complexity_score': self._calculate_complexity_score(code)
        }
        return analysis
    
    def _calculate_complexity_score(self, code: str) -> int:
        """Calculate a simple complexity score for the code."""
        score = 0
        score += code.count('for ') * 2  # Loops add complexity
        score += code.count('if ') * 1   # Conditionals add complexity
        score += code.count('def ') * 1  # Functions add complexity
        return score
    
    def _create_learning_question(self, code: str, coaching_state: CoachingState, 
                                analysis: Dict[str, Any]) -> Tuple[str, CoachingMode]:
        """Create an appropriate learning question."""
        # Select the best question for this code
        question = self.question_selector.select_question_for_code(code, coaching_state)
        
        # Create coaching interaction
        interaction = CoachingInteraction(
            interaction_id=str(uuid.uuid4()),
            mode=CoachingMode.QUESTION,
            content=self._format_question_message(question),
            question=question
        )
        
        # Update coaching state
        coaching_state.current_interaction = interaction
        coaching_state.total_questions_asked += 1
        
        return interaction.content, CoachingMode.QUESTION
    
    def _create_nudge(self, code: str, analysis: Dict[str, Any], coaching_state: CoachingState) -> Tuple[str, CoachingMode]:
        """Create a direct nudge to help the user improve their code, or congratulate if the main issue is solved."""
        main_issue = coaching_state.main_issue

        # Map issue keys to analysis flags
        issue_flags = {
            'has_iterrows': analysis.get('has_iterrows', False),
            'has_string_concat': analysis.get('has_string_concat', False),
            'has_nested_loops': analysis.get('has_nested_loops', False),
            # Add more as needed
        }

        if analysis['has_iterrows']:
            nudge = """üéØ **Optimization Opportunity**: I notice you're using `df.iterrows()` which is quite slow for large datasets. 

    **Hint**: Pandas shines with vectorized operations that work on entire columns at once. Try replacing your loop with direct column operations like `df['column1'] * df['column2']`.

    Would you like to try optimizing this part of your code?

    üí° **Learning Options:** Want a more specific hint, or prefer to explore this with a hands-on question first?"""
        
        elif analysis['has_string_concat']:
            nudge = """üéØ **Performance Tip**: Building strings with `+=` in a loop can be slow for large amounts of text.

    **Hint**: Consider collecting your strings in a list and using `''.join(list)` at the end for better performance.

    Want to give it a try?

    üí° **Learning Options:** Need a hint on the exact syntax, or want to explore this concept with another question?"""
        
        elif analysis['complexity_score'] > 6:
            nudge = """üéØ **Readability Opportunity**: Your code is getting a bit complex. 

    **Hint**: Consider breaking it into smaller functions with descriptive names. This makes it easier to test and understand.

    What do you think about refactoring this?

    üí° **Learning Options:** Ask for guidance on refactoring approaches, or try a question about code organization principles?"""
        
        else:
            nudge = """üéØ **Good work!** Your code looks functional. Let's explore some potential optimizations or improvements together.

    What aspect would you like to focus on: performance, readability, or error handling?

    üí° **Learning Options:** Want me to ask you a targeted question about optimization, or prefer to dive straight into improving the code?"""
        
        return nudge, CoachingMode.NUDGE
    
    def _format_question_message(self, question: LearningQuestion) -> str:
        """
        FIXED: Format a learning question for display in chat.
        Enhanced error handling and cloud deployment compatibility.
        """
        # VALIDATION: Ensure question object is properly formed
        if not question:
            print("ERROR: _format_question_message received None question")
            return "**Error:** Question object is None. Please try submitting your code again."
        
        # VALIDATION: Check required attributes
        required_attrs = ['question_type', 'title', 'question_text']
        for attr in required_attrs:
            if not hasattr(question, attr):
                print(f"ERROR: Question missing required attribute: {attr}")
                return f"**Error:** Question object missing {attr}. Please try submitting your code again."
        
        # SAFE DEBUG LOGGING: Avoid import-dependent failures
        def safe_debug_log(msg: str):
            """Safe debug logging that won't fail on import issues."""
            try:
                # Try relative import first
                from .session_manager import add_debug_message
                add_debug_message(msg)
            except ImportError:
                try:
                    # Try absolute import
                    from core.session_manager import add_debug_message
                    add_debug_message(msg)
                except ImportError:
                    # Fall back to print for debugging
                    print(f"DEBUG: {msg}")
            except Exception:
                # Silent fallback - don't let debug logging break functionality
                print(f"DEBUG: {msg}")
        
        try:
            # BUILD MESSAGE STEP BY STEP with validation at each step
            message_parts = []
            
            # 1. Header with validation
            if hasattr(question, 'title') and question.title:
                header = f"üìö **{question.title}**"
                if hasattr(question, 'question_type') and hasattr(question.question_type, 'value'):
                    header += f" ({question.question_type.value.upper()})"
                message_parts.append(header)
            
            # 2. Question text with validation
            if hasattr(question, 'question_text') and question.question_text:
                message_parts.append(question.question_text)
            else:
                safe_debug_log("‚ùå Question text is missing or empty")
                return "**Error:** Question text is missing. Please try submitting your code again."
            
            # 3. Toy code if present
            if hasattr(question, 'toy_code') and question.toy_code:
                message_parts.append(f"```python\n{question.toy_code}\n```")
            
            # 4. MCQ OPTIONS - CRITICAL SECTION with enhanced validation
            if (hasattr(question, 'question_type') and 
                hasattr(question.question_type, 'value') and
                question.question_type.value == 'mcq'):
                
                safe_debug_log("üîç Processing MCQ options...")
                
                # Validate options exist
                if not hasattr(question, 'options'):
                    safe_debug_log("‚ùå MCQ question missing options attribute")
                    message_parts.append("**Error:** Multiple choice options missing.")
                elif not question.options:
                    safe_debug_log("‚ùå MCQ question has empty options list")
                    message_parts.append("**Error:** Multiple choice options list is empty.")
                elif len(question.options) == 0:
                    safe_debug_log("‚ùå MCQ question has zero options")
                    message_parts.append("**Error:** No multiple choice options available.")
                else:
                    safe_debug_log(f"‚úÖ MCQ has {len(question.options)} options")
                    
                    # Build options section
                    options_section = ["**Options:**"]
                    
                    for i, option in enumerate(question.options):
                        if not option:
                            safe_debug_log(f"‚ùå Option {i} is None")
                            continue
                        
                        letter = chr(ord('A') + i)
                        
                        # Safely get option text
                        option_text = ""
                        if hasattr(option, 'text') and option.text:
                            option_text = option.text
                        else:
                            safe_debug_log(f"‚ùå Option {i} missing text attribute")
                            option_text = f"[Option {letter} - text missing]"
                        
                        options_section.append(f"{letter}) {option_text}")
                    
                    # Add options to message
                    message_parts.extend(options_section)
                    message_parts.append("**üí¨ How to respond:** Type just the letter (A, B, C, or D)")
                    
                    safe_debug_log(f"‚úÖ MCQ formatted successfully with {len(question.options)} options")
            
            # 5. Response instructions for other question types
            elif hasattr(question, 'question_type'):
                if (hasattr(question.question_type, 'value') and 
                    question.question_type.value == 'tf'):
                    message_parts.append("**üí¨ How to respond:** Type 'True' or 'False'")
                elif (hasattr(question.question_type, 'value') and 
                      question.question_type.value == 'toy_example'):
                    message_parts.append("**üí¨ How to respond:** Tell me which option you think is better and briefly why")
                elif (hasattr(question.question_type, 'value') and 
                      question.question_type.value == 'spot_bug'):
                    message_parts.append("**üí¨ How to respond:** Describe what you think the issue is")
                elif (hasattr(question.question_type, 'value') and 
                      question.question_type.value == 'what_if'):
                    message_parts.append("**üí¨ How to respond:** Explain what you think would happen")
                else:
                    message_parts.append("**üí¨ How to respond:** Share your thoughts")
            
            # 6. Build final message
            final_message = "\n\n".join(message_parts)
            
            # FINAL VALIDATION: Ensure message is substantial
            if len(final_message.strip()) < 50:
                safe_debug_log("‚ùå Final message too short, potential formatting failure")
                return f"**Question:** {question.question_text}\n\n**How to respond:** Share your thoughts"
            
            safe_debug_log(f"‚úÖ Question formatted successfully: {len(final_message)} characters")
            return final_message
            
        except Exception as e:
            # ROBUST ERROR HANDLING: Log error but provide fallback
            error_msg = str(e)
            safe_debug_log(f"‚ùå Question formatting error: {error_msg}")
            
            # Try to provide basic question at minimum
            try:
                if hasattr(question, 'question_text') and question.question_text:
                    return f"**Question:** {question.question_text}\n\n**How to respond:** Share your thoughts"
                else:
                    return "**Error:** Question formatting failed. Please try submitting your code again."
            except:
                return "**Error:** Critical question formatting failure. Please try submitting your code again."
        
    def _evaluate_answer(self, user_answer: str, question: LearningQuestion) -> Tuple[bool, str]:
        """
        Evaluate user's answer to a question.
        
        Returns:
            Tuple of (is_correct, feedback_message)
        """
        user_answer = user_answer.strip().lower()
        
        if question.question_type == QuestionType.MULTIPLE_CHOICE:
            return self._evaluate_mcq_answer(user_answer, question)
        
        elif question.question_type == QuestionType.TRUE_FALSE:
            return self._evaluate_tf_answer(user_answer, question)
        
        else:
            return self._evaluate_open_answer(user_answer, question)
    
    def _evaluate_mcq_answer(self, user_answer: str, question: LearningQuestion) -> Tuple[bool, str]:
        """Evaluate multiple choice question answer."""
        correct_letter = question.correct_answer.lower()
        is_correct = user_answer == correct_letter
        
        # Find the selected option for feedback
        option_map = {chr(ord('a') + i): opt for i, opt in enumerate(question.options)}
        selected_option = option_map.get(user_answer)
        
        if is_correct:
            feedback = f"‚úÖ **Correct!** Great job!\n\n{question.explanation}"
        else:
            if selected_option:
                feedback = f"‚ùå **Not quite.** {selected_option.explanation}\n\n**Correct answer:** {question.correct_answer}) {option_map[correct_letter].text}\n\n{question.explanation}"
            else:
                feedback = f"‚ùå **Invalid answer.** Please choose A, B, C, or D.\n\n**Correct answer:** {question.correct_answer}) {option_map[correct_letter].text}"
        
        return is_correct, feedback
    
    def _evaluate_tf_answer(self, user_answer: str, question: LearningQuestion) -> Tuple[bool, str]:
        """Evaluate true/false question answer."""
        correct_answer = question.correct_answer.lower()
        is_correct = user_answer in ['true', 't'] if correct_answer == 'true' else user_answer in ['false', 'f']
        
        if is_correct:
            feedback = f"‚úÖ **Correct!** {question.explanation}"
        else:
            feedback = f"‚ùå **Not quite.** The correct answer is **{question.correct_answer}**.\n\n{question.explanation}"
        
        return is_correct, feedback
    
    def _evaluate_open_answer(self, user_answer: str, question: LearningQuestion) -> Tuple[bool, str]:
        """Evaluate open-ended question answer."""
        # For open-ended questions, be more generous with partial credit
        correct_keywords = question.correct_answer.lower().split()
        has_keywords = any(keyword in user_answer for keyword in correct_keywords)
        
        if has_keywords:
            feedback = f"‚úÖ **Good thinking!** {question.explanation}"
        else:
            feedback = f"üí≠ **Interesting perspective.** Here's what I was thinking: {question.explanation}"
        
        return has_keywords, feedback